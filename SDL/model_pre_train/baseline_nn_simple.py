# baseline_nn_simple.py - ç®€åŒ–ç‰ˆï¼Œåªä½¿ç”¨pHå’Œä½“ç§¯ä¿¡æ¯
import math, random, os, warnings
import logging
import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import glob
warnings.filterwarnings("ignore")

torch.manual_seed(42)
np.random.seed(42)

# ---------- æ—¥å¿— ----------
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "baseline_nn_simple.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[logging.FileHandler(LOG_FILE, encoding="utf-8"), logging.StreamHandler()]
)

# ---------- å½’ä¸€åŒ–å±‚ ----------
class NormalizationLayer(nn.Module):
    def __init__(self, mean, std):
        super().__init__()
        self.register_buffer('feature_mean', mean.clone().detach())
        self.register_buffer('feature_std', std.clone().detach())
        # é¿å…é™¤é›¶
        self.feature_std = torch.where(self.feature_std == 0, 
                                     torch.ones_like(self.feature_std), 
                                     self.feature_std)

    def forward(self, x):
        return (x - self.feature_mean) / self.feature_std

# ---------- ç®€åŒ–æ¨¡å‹ï¼ˆåªä½¿ç”¨ä½“ç§¯ä¿¡æ¯ï¼‰ ----------
class SimpleBaselineNN(nn.Module):
    def __init__(self, input_dim=1, feature_mean=None, feature_std=None):
        super().__init__()
        
        # å½’ä¸€åŒ–å±‚
        if feature_mean is not None and feature_std is not None:
            self.normalize = NormalizationLayer(feature_mean, feature_std)
        else:
            self.normalize = None

        # ç®€åŒ–ç½‘ç»œç»“æ„ï¼Œåªé¢„æµ‹pH
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        if self.normalize is not None:
            x = self.normalize(x)
        return self.net(x).squeeze(-1)

# ---------- ç®€åŒ–æ•°æ®é›†ï¼ˆåªä½¿ç”¨ä½“ç§¯ï¼‰ ----------
class SimpleBaselineDataset(Dataset):
    def __init__(self, csv_files, scaler=None):
        self.records = []
        
        print("ğŸ”§ åŠ è½½ç®€åŒ–æ•°æ®é›†...")
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                filename = os.path.basename(csv_file)
                
                for _, row in df.iterrows():
                    # åªä½¿ç”¨ä½“ç§¯ä½œä¸ºç‰¹å¾
                    volume = row["Volume_mL"]
                    ph = row["pH"]
                    
                    record = {
                        "volume": volume,
                        "ph": ph,
                        "file": filename
                    }
                    self.records.append(record)
                
                print(f"ğŸ“Š åŠ è½½: {filename} - {len(df)} ä¸ªæ•°æ®ç‚¹")
                
            except Exception as e:
                print(f"âŒ åŠ è½½ {csv_file} å¤±è´¥: {e}")
        
        # å‡†å¤‡ç‰¹å¾
        features = np.array([[r["volume"]] for r in self.records], dtype=np.float32)
        targets = np.array([r["ph"] for r in self.records], dtype=np.float32)
        
        # å½’ä¸€åŒ–
        if scaler is None:
            self.scaler = StandardScaler().fit(features)
        else:
            self.scaler = scaler
            
        self.features = features  # åŸå§‹ç‰¹å¾
        self.targets = targets

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        return {
            "features": torch.tensor(self.features[idx], dtype=torch.float32),
            "ph": torch.tensor([self.targets[idx]], dtype=torch.float32),
            "file": self.records[idx]["file"]
        }

# ---------- è®­ç»ƒä¸€ä¸ª epoch ----------
def run_epoch(loader, model, optimizer, loss_fn, training=True, device="cpu"):
    total, mse = 0, 0.0
    model.train() if training else model.eval()
    for batch in tqdm(loader, desc="train" if training else "val"):
        features = batch["features"].to(device)
        ph_true = batch["ph"].to(device)
        
        if training:
            optimizer.zero_grad()
            
        with torch.set_grad_enabled(training):
            ph_pred = model(features)
            loss = loss_fn(ph_pred, ph_true.squeeze())
            
            if training:
                loss.backward()
                optimizer.step()
                
        total += ph_true.size(0)
        mse += loss.item() * ph_true.size(0)
        
    return math.sqrt(mse / total)

# ---------- ä¸»å…¥å£ ----------
if __name__ == "__main__":
    # è¶…å‚æ•°
    BATCH_SIZE = 512
    EPOCHS = 50
    LR = 1e-3
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    os.makedirs("ckpt", exist_ok=True)

    # è®¾ç½®éšæœºç§å­
    def set_seed(s=42):
        random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)
    set_seed()

    # 1. æŸ¥æ‰¾CSVæ–‡ä»¶
    CSV_DIR = r"E:\mypython\SDL\train_csv"  # ä½¿ç”¨çœŸå®æ•°æ®
    csv_files = glob.glob(os.path.join(CSV_DIR, "*.csv"))
    
    if not csv_files:
        raise FileNotFoundError(f"No CSV found in {CSV_DIR}")
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

    # 2. åˆ›å»ºæ•°æ®é›†
    all_ds = SimpleBaselineDataset(csv_files, scaler=None)
    
    # æŒ‰æ ·æœ¬æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
    all_records = all_ds.records
    all_features = all_ds.features
    all_targets = all_ds.targets

    # éšæœºæ‰“ä¹±å¹¶åˆ’åˆ†
    indices = np.random.permutation(len(all_records))
    split_idx = int(0.8 * len(all_records))

    train_indices = indices[:split_idx]
    val_indices = indices[split_idx:]

    # åˆ›å»ºè®­ç»ƒé›†
    train_ds = SimpleBaselineDataset([], scaler=all_ds.scaler)
    train_ds.records = [all_records[i] for i in train_indices]
    train_ds.features = all_features[train_indices]
    train_ds.targets = all_targets[train_indices]

    # åˆ›å»ºéªŒè¯é›†
    val_ds = SimpleBaselineDataset([], scaler=all_ds.scaler)
    val_ds.records = [all_records[i] for i in val_indices]
    val_ds.features = all_features[val_indices]
    val_ds.targets = all_targets[val_indices]

    print(f"è®­ç»ƒé›†: {len(train_ds)} æ ·æœ¬, éªŒè¯é›†: {len(val_ds)} æ ·æœ¬")

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

    # 4. æ¨¡å‹
    train_mean = torch.from_numpy(all_ds.scaler.mean_).float().to(DEVICE)
    train_std = torch.from_numpy(all_ds.scaler.scale_).float().to(DEVICE)
    
    model = SimpleBaselineNN(input_dim=1, feature_mean=train_mean, feature_std=train_std).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    loss_fn = nn.MSELoss()

    # 5. ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    np.save("ckpt/baseline_simple_mean.npy", all_ds.scaler.mean_)
    np.save("ckpt/baseline_simple_std.npy", all_ds.scaler.scale_)
    logging.info(f"ç®€åŒ–æ¨¡å‹å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜")

    # 6. è®­ç»ƒå¾ªç¯
    best_val_rmse = float('inf')
    for epoch in range(1, EPOCHS + 1):
        train_rmse = run_epoch(train_loader, model, optimizer, loss_fn, True, DEVICE)
        val_rmse = run_epoch(val_loader, model, optimizer, loss_fn, False, DEVICE)
        scheduler.step()
        
        logging.info(f"Epoch {epoch:02d} | Train RMSE {train_rmse:.4f} | Val RMSE {val_rmse:.4f}")
        
        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            torch.save({
                'model_state_dict': model.state_dict(),
                'scaler_mean': all_ds.scaler.mean_,
                'scaler_std': all_ds.scaler.scale_,
                'input_dim': 1,
                'val_rmse': best_val_rmse,
                'epoch': epoch
            }, "ckpt/baseline_simple_best.pt")
            logging.info(f"*** æ–°æœ€ä½³ val RMSE: {best_val_rmse:.4f} ***")

    logging.info(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯RMSE: {best_val_rmse:.4f}")